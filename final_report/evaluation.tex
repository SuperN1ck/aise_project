\chapter{Evaluation}
\label{cha:evaluation}

After successfully implementing our MOOP extension we started to conduct various experiments.

\section{Setup}
\label{sec:experimental_setup}

We used the previous serialized data (see \cref{sec:serializing_data}) from the three different projects that were provided with the Github repository\footnote{\href{https://github.com/SERG-Delft/evosql}{https://github.com/SERG-Delft/evosql}} by the authors of the original EvoSQL paper. 

As previously explained in \cref{sec:coverage_target_extraction} for each query we extract multiple coverage targets. In \cref{fig:cov_targets_dist} we report the distribution of all coverage targets from all three projects.

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			width = .7\linewidth,
			ybar, 
			bar width = 4pt,
			xlabel = Coverage Targets for Query,
			ylabel = Amount,
			xmax = 30,
			xtick = {0, 5, 10, 15, 20, 25, 28}]
			\addplot table[x=pathnumbers, y=count, col sep=comma] {assets/coverage_targets_distribution.csv};
		\end{axis}
	\end{tikzpicture}
	\caption{Distribution of coverage targets (i.e. Objectives) in our whole test query dataset}
	\label{fig:cov_targets_dist}
\end{figure}

As seen in the data distribution a lot of our queries have three coverage targets which need to be covered in order to say the query is solved.

The main reason why so many queries have three targets is because if we have single condition in our query, for instance:
\begin{verbatim}
SELECT user_id AS 'userId' FROM autofollow WHERE entity_type = 'Account'
\end{verbatim}

The three possible targets are:
\begin{verbatim}
SELECT "user_id" AS "userId" FROM "autofollow" WHERE NOT ("entity_type" = 'Account')
SELECT "user_id" AS "userId" FROM "autofollow" WHERE ("entity_type" = 'Account')
SELECT "user_id" AS "userId" FROM "autofollow" WHERE ("entity_type" IS NULL)
\end{verbatim}

In our experiment we measured the average time it takes to solve a query with a given amount of targets. To ensure we don't run into unsolvable targets we limit the maximum execution to 30 mins.

In the case of the original implementation we sum up the amount of time needed to solve each target individually. This sum then forms the total execution time for the single-objective optimization. When measuring the multi-objective optimization we only need to measure the over all execution time.

\section{Results}
\label{sec:results}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			width = .7\linewidth,
			bar width = 4pt,
			xlabel = Coverage Targets for Query,
			ylabel = Average Execution Time,
			xtick = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15}]
			\addplot table[x=Coverage Targets, y=Original GA, col sep=comma] {assets/execution_time.csv};
			\addplot table[x=Coverage Targets, y=MOO, col sep=comma] {assets/execution_time.csv};
		\end{axis}
	\end{tikzpicture}
	\caption{Average execution time for each covered target. For the last data point (15 coverage targets, our MOO was able to only cover 3 targets in the given time frame of 30 mins.)}
	\label{fig:execution_time_results}
\end{figure}

To our personal surprise the time to fully cover a query increased for our multi-objective optimization implementation with increasing amount of targets to cover, even hitting time outs when having a lot of coverage targets (objectives).

\section{Failure Analysis}

If we look at \cref{tbl:ratio_coverage_targets} we can see that the actual ratio between execution time is not staying constant. This lead to the conclusion on our side that with increasing amount of coverage targets our multi-objective algorithm performs worse.

\begin{table}
	\centering
	\label{tbl:ratio_coverage_targets}
	\caption{Ratio of execution time between original single-objective optimization and our implemented multi-objective optimization (if $>1$: multi-objective optimization is slower)}
	\begin{tabular}{c|c|c|c|c|c|c|c|c}
		Coverage Targets & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 
		\hline
		Ratio (MOO/GA) & 2.259 & 1.587 & 1.501 & 1.581 & 4.265 & 5.860 & 6.816 & 13.463
	\end{tabular} 
\end{table}

To further investigate why this happens we took a look at the execution time of each components. Here we could narrow it down that our fitness calculation becomes worse with increasing amount of coverage targets.

Initially, this makes sense as we have to calculate the fitness "amount of coverage targets" times for each individual. But as we have execute the optimization algorithm only once in the case of multi-objective optimization and not "amount of coverage targets" times it should even out in the end.

Next, we found out that the actual time for calculating the fitness for one coverage target increases as well with having more coverage targets. When looking at this aspect, the only thing that changes with increasing amount of coverage targets is the maximum amount of rows we can have in an individual.

%As previously described in ..?
We have to scale the maximum amount of rows based on the amount of coverage targets as our individuals need to have enough "capacity" to potentially cover all targets. We do so by setting the capacity to
\begin{verbatim}
	max_rows = max_rows_per_target * amount_coverage_targets
\end{verbatim}
We adapted the given \verb|max_rows_per_target| from the single objective optimization and set it to 4.

\subsection{Why Does the Capacity Matters?}
\label{sec:why_capacity_matters}
We will now analyze why the fitness calculation becomes more costly when the amount of rows increase per table.

\subsubsection{Theoretical Analysis}

Assuming we have a simple example with two tables, if we now execute a query on these two tables with the form of like \verb|WHERE table1.field == "Foo" AND table2.field == "Bar"| we have to calculate the minimum fitness.

Abstractly this could be described in the following way
\begin{verbatim}
    min_fitness = Null
    foreach row1 in table1:
        foreach row2 in table2:
            fitness = query.get_distance(row1, row2)
            if fitness < min_fitness:
                min_fitness
    return min_fitness
\end{verbatim}
So in this hypothetical case, where no optimization are applied we could boil the total amount of query plan analyzations (in \verb|get_distance|) can be calculated by
\begin{equation}
	\label{eqn:comparisons_for_fitness}
	c(T) = \prod_{t \in T} |t|
\end{equation}
where $T$ is the set of all tables and the norm $|t|$ is the amount of rows in a table.

For our convenience we set the maximum amount of rows $m_r$ equal for all tables. Therefore we can construct our worst case scenario/an upper bound by assuming all tables are filled completely we have the worst case scenario for \cref{eqn:comparisons_for_fitness} of 
\begin{equation}
	\label{eqn:comparisons_for_fitness_worst}
	c(T) = {m_r}^{|T|}
\end{equation} 
where $|T|$ is the total number of tables.

If we now look at a constructed example with
\begin{itemize}
	\item 27 coverage targets,
	\item 6 tables
	\item 4 max rows per target
\end{itemize}

In the single objective case the total amount of comparisons is
\begin{equation}
	\underbrace{27}_{\textrm{Coverage Targets/Single Optimization Executions}} ~\cdot~ \underbrace{4^6}_{\textrm{\Cref{eqn:comparisons_for_fitness_worst}}} ~\cdot~ generations ~\cdot~ individuals
\end{equation}
while in the multi-objective case we construct the amount of comparisons the following way
\begin{equation}
	\underbrace{1}_{Optimization Execution} ~\cdot~ \underbrace{(27 \cdot 4)^6}_{\textrm{\Cref{eqn:comparisons_for_fitness_worst}}} ~\cdot~ generations ~\cdot~ individuals.
\end{equation}
There in this simple example the ratio of comparisons is
\begin{equation}
	\label{eqn:theoretical_ratio_row_comparisons}
	\frac{\textrm{Comparisons MOO}}{\textrm{Comparisons SO}} = \frac{(27 \cdot 4)^6}{27 \cdot 4^6} = 14,348,907
\end{equation}
Of course this example is constructed and does not reflect all the cases we have in our distribution (see \cref{fig:cov_targets_dist}). But even if we lower the amount of coverage targets to 3, this ratio would be 243.

\subsubsection{Empircal Study}
In order to check our assumptions we recorded the actual amount of comparisons we did when calculating the fitness. We identified the following piece of code\footnote{ \href{https://github.com/SuperN1ck/aise_project/blob/b1ab9703dfb31535c2a791bd9ce74edc17c34829/evosql/instrumented-hsqldb/src/main/java/genetic/Instrumenter.java\#L427}{See} for more context} to be vital for our study:
\begin{lstlisting}
for (ComparisonRow c : iterStore.getRows()) {
	try {
		currentDistance = c.getDistance();
		/*if (currentDistance == 0) {
		c.getDistance();
	}*/
	} catch (OperationNotSupportedException e) {
		log.error(e);
		currentDistance = Double.MAX_VALUE;
	}
	
	[...]
}
\end{lstlisting}
Here we use the size of \lstinline|iterStore.getRows()| in order to determine the total amount of executed fitness plans. If we assume our instrumentation does not prior optimization and we have full rows this value would be calculated according to \cref{eqn:comparisons_for_fitness}.

We recorded the size of the row-array for two different queries the properties and results can be found in \cref{tbl:empirical_comparison_eval}. Here we can see that luckily our previously calculated ratio of $14,348,907$ (See \cref{eqn:theoretical_ratio_row_comparisons}) was not fully reached.

\begin{table}
	\centering
	\label{tbl:empirical_comparison_eval}
	\caption{Empirical comparison of the executed query plans}
	\begin{tabular}{c|c|c|c||c|c|c|c|c|c}
		\multirow{2}{*}{Query} & \multirow{2}{*}{Tables} & \multirow{2}{*}{Targets} & \multirow{2}{*}{Max Rows} & \multicolumn{2}{c|}{Single-Objective} & \multicolumn{2}{c|}{Multi-Objective} & \multicolumn{2}{c}{Factor/Magnitude} \\
		\hhline{*{4}{~}*{6}{-}}
		& & & & Average & Max & Average & Max & Average & Max \\ 
		\hhline{*{10}{=}}
		Simple & 3 & 11 & 4 & 4.82 & 21.00 & 1273.99 & 9246.00 & 264.21 & 440.29 \\ 
		\hline 
		Complex & 6 & 27 & 4 & 14.17 & 460.00 & 16619.06 & 863391.00 & 1173.07 & 1876.94
	\end{tabular} 
\end{table}
